# no subword model set by default
# default optimizer settings are used
# use on the fly tokenization with sentencepiece

## Where the vocab(s) will be written
src_vocab: data/vocab.src
tgt_vocab: data/vocab.tgt

# Tokenisation options
src_subword_type:
src_subword_model:
tgt_subword_type:
tgt_subword_model:
# Number of candidates for SentencePiece sampling
subword_nbest: 20
# Smoothing parameter for SentencePiece sampling
subword_alpha: 0.1

# Specific arguments for pyonmttok
src_onmttok_kwargs: "{'mode': 'none', 'spacer_annotate': True}"
tgt_onmttok_kwargs: "{'mode': 'none', 'spacer_annotate': True}"

# Corpus opts:
data:
  # (required for train run type).
  corpus_1:
    path_src: data/src-train.txt
    path_tgt: data/tgt-train.txt
    transforms: [onmt_tokenize, filtertoolong]
    weight: 1
  valid:
    path_src: data/src-val.txt
    path_tgt: data/tgt-val.txt
    transforms: [onmt_tokenize]

# Transformer Model Hyperparameters
save_data: data/
save_model: models/model
save_checkpoint_steps: 20000
valid_steps: 2000
train_steps: 100000
seed: 5151
report_every: 1000
keep_checkpoint: 5
rnn_size: 512

# Logging
tensorboard: true
log_file: data/training_log.txt

# Train on a single GPU
world_size: 1
gpu_ranks: [0]
early_stopping: 4

